---
output:
  revealjs::revealjs_presentation:
    css: ./css/revealOpts.css
    center: no
    theme: solarized
    self_contained: no
    transition: slide
---

```{r include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(
  echo = FALSE
  , message = FALSE
)
```

# Linear Blues

##

> "...magical dreamlike logic ..."

## What do these things have in common?

##

> - Amy Winehouse - Rehab
> - U2 - Still Haven't Found What I'm Looking For
> - Tracy Chapman - Give Me One Reason
> - Prince - Purple Rain
> - The Clash - Should I Stay or Should I Go?
> - KC & The Sunshine Band - Boogie Shoes
> - Steelers Wheel - Stuck in the Middle With You
> - Pink Floyd - Money
> - Led Zeppelin - Rock and Roll
> - James Brown - I Got You (I Feel Good)
> - The Beatles - Can't Buy Me Love
> - Johnny Cash - Folsom Prison Blues
> - Miles Davis - All Blues
> - Chuck Berry - Johnny B. Goode
> - Robert Johnson - Sweet Home Chicago

## The pattern

```{r }
munge_blues <- function(tbl) {
  tbl %>% 
    tidyr::gather(bar, chord, -row) %>% 
    mutate(bar = gsub('bar_', "", bar))
}

plt_blues <- function(tbl) {
  tbl %>% 
    ggplot(aes(bar, row)) +
    geom_tile(aes(fill = chord), color = 'white') + 
    geom_text(aes(label = chord), color = 'white', size = 10) +
    theme_void() + 
    theme(
      legend.position = 'none'
    ) +
    scale_y_reverse() +
    scale_fill_manual(values = c('#0000FF', '#CC8614', '#7914CC'))
}
```

```{r}
tbl_blues <- tribble(
  ~row, ~bar_1, ~bar_2, ~bar_3, ~bar_4
  , 1 , 'I', 'I', 'I', 'I'
  , 2 , 'IV', 'IV', 'I', 'I'
  , 3 , 'V', 'V', 'I', 'I'
) %>% 
  munge_blues()
tbl_blues %>% 
  plt_blues()
```

## The variations

```{r}
tribble(
  ~row, ~bar_1, ~bar_2, ~bar_3, ~bar_4
  , 1 , 'I', 'I', 'I', 'I'
  , 2 , 'IV', 'IV', 'I', 'I'
  , 3 , 'V', 'IV', 'I', 'I'
) %>% 
  munge_blues() %>% 
  plt_blues()
```

## The variations

```{r}
tribble(
  ~row, ~bar_1, ~bar_2, ~bar_3, ~bar_4
  , 1 , 'I', 'I', 'I', 'I'
  , 2 , 'IV', 'IV', 'I', 'I'
  , 3 , 'V', 'IV', 'I', 'V'
) %>% 
  munge_blues() %>% 
  plt_blues()
```

# The linear pattern

## The linear pattern

<div class='big-math'>
$$\mu_i=\beta_0 + \sum_{j=1}^{p}{(\beta_j * x_{ij})}+\epsilon_i$$
</div>

## The linear pattern

<div class='big-math'>
$$\mu_i=\beta_0 + \sum_{j=1}^{p}{(\beta_j * x_{ij})}+\epsilon_i$$
</div>

_i_ -> sample point

_j_ -> predictor

$E[\epsilon_i]=0$

$Var[\epsilon_i]=\sigma$

$Cov(\epsilon_i,\epsilon_j)=0, \forall i \neq j$

## Variations

<div class='big-math'>
$$\epsilon_i=\mu_i - \beta_0 - \sum_{j=1}^{p}{(\beta_j * x_{ij})}$$
</div>

## Variation

Knowing that $E[\epsilon_i]=0$, we can presume normality and use maximum likelihood to estimate $\sigma$. 

We just need to use values of $\beta_0$ and $\beta_j$ that will get $\epsilon$ close to zero.

Let's assume that we've done that.

## Original Data

$\beta_0$ = 10, $\beta_1$ = 1.5

```{r }
set.seed(123)
sims <- 500
b_0 <- 10
b_1 <- 1.5
tbl_linear <- tibble(
    x = runif(sims, -10, 10)
  , e = rnorm(sims, 0, 2) 
) %>% 
  mutate(
    y = b_0 + b_1 * x + e
  )
```

```{r fig.height=5}
tbl_linear %>% 
  ggplot(aes(x, y)) + 
  geom_point()
```

## Determine 

```{r}
eval_epsilon <- function(parms) {
  est_epsilon <- tbl_linear %>% 
    mutate(est_epsilon = y - parms[1] - parms[2] * x) %>% 
    pull(est_epsilon)
  est_epsilon^2  %>% mean()
}

fits <- optim(
    par = c(10, 1.5)
  , eval_epsilon
)
fits

eval_epsilon(fits$par)
```

## Subtract $\sum_{j=1}^{p}{(\beta_j * x_{ij})}$

```{r}
fit <- lm(y ~ 1 + x, data = tbl_linear)
b_0_fit <- coef(fit)[1]
b_1_fit <- coef(fit)[2]
tbl_linear <- tbl_linear %>% 
  mutate(
    y_adj = y - b_1_fit * x
  )
```

```{r}
tbl_linear %>% 
  ggplot(aes(x, y_adj)) + 
  geom_point()
```

## Subtract intercept

```{r}
tbl_linear <- tbl_linear %>% 
  mutate(
      y_adj = y_adj - b_0_fit
    , y_adj_2 = y - b_0 - b_1 * x
  )

tbl_linear %>% 
  ggplot(aes(x, y_adj)) + 
  geom_point()
```

##

<div class='left'>
```{r }
tbl_linear %>% 
  ggplot(aes(x, y)) + 
  geom_point()
```
</div>

<div class='right'>
```{r}
tbl_linear %>% 
  ggplot(aes(x, y_adj)) + 
  geom_point()
```
</div>

## We're seeking noise

We presume that the observation is a linear transform of a random variable. 

We've assumed a normal distribution, but do we have to?

## Logistic

$$\epsilon_i \text{~} ~ logistic(0,1)$$

$$z_i=\beta_0 + \sum_{j=1}^{p}{\beta_j * x_{ij}}+\epsilon_i$$

$$y_i =
  \begin{cases}
    1  & \text{if } z_i > 0 \\
    0  & \text{if } z_i < 0
  \end{cases}$$

##

```{r echo=FALSE}
tbl_linear <- tbl_linear %>% 
  mutate( 
      e_logis = rlogis(sims)
    , latent = b_0 + b_1 * x + e
    , y_logis = as.integer(latent > 0)
  )
```

```{r }
tbl_linear %>% 
  ggplot(aes(x, y_logis)) + 
  geom_point()
```

## How do we fit this with a linear model?

$$f(x)=\frac{exp(x)}{exp(x) + 1}$$

So, just take $\beta_0 + \sum_{j=1}^{p}{\beta_j * x_{ij}}$ and transform it.

##

```{r }
tbl_linear %>% 
  ggplot(aes(x, y_logis)) + 
  geom_point() + 
  geom_smooth(method = glm,  method.args = list(family = "binomial"))
```

##

```{r echo=TRUE}
fit_logistic <- glm(
    y_logis ~ 1 + x
  , data = tbl_linear
  , family = binomial
)

coef(fit_logistic)
```

## The process

So we can use the **same** process to create a logistic regression!

1. Random noise -> 
2. Linear transform -> 
3. Transform that result (optional) ->
4. Observation

Gaussian and logistic are the only two distributions that I know of where we can get away with this. For other distributions ...

## We don't really start with noise

Rather than assuming we start with noise which is transformed, we start with the observation. This is reality. 

$$E[Y_i]=\mu_i=\beta_0 + \sum_{j=1}^{p}{\beta_j * x_{ij}}$$

We lost $\epsilon$. Early uses of linear models were very focused on this, because $\epsilon$ captured _measurement error_. Later uses were focused on _probability distributions_.

## Poisson

```{r}
sims_poisson <- 5e3
tbl_poisson <- tibble(
  x = sample(100:1000, size = sims_poisson, replace = TRUE)
) %>% 
  mutate(
      eta = b_0 + b_1 * x 
    , y = rpois(sims_poisson, eta)
  )
```

```{r}
tbl_poisson %>% 
  ggplot(aes(x, y)) +
  geom_hex()
```

## How do we assess coefficients?

Earlier, we synthesized data. So, we knew exactly which coefficients generated the observations. Now what?

## For a GLM

<div class='big-math'>
$$\frac{\partial{l(y;\beta)}}{\partial\beta_j}=\sum_{i=1}^n{\frac{(y_i-\mu_i)x_{ij}}{a_i(\phi)b''(\theta_i)g'(\mu_i)}}$$
</div>

## Poisson
<div class='big-math'>
$$\sum_{i=1}^{n}{(y_i-\mu_i)x_{ij}}$$
</div>

BUT

<div class='big-math'>
$$\mu_i=\beta_0+\sum{\beta_jx_{ij}}$$
</div>

## Poisson

$$\sum_{i=1}^{n}{(y_i-\beta_0+\beta_jx_{ij})x_{ij}}$$

## Does my coefficient explain my data?

```{r}
fit_poisson <- glm(y ~ 1 + x, data = tbl_poisson, family = poisson(link = 'identity'))
summary(fit_poisson)
```

## Coefficient significance

* Actually, just testing if the coefficient is different than zero
* Regressing cumulative loss against prior cumulative, coefficients are _always_ significant

## Reserves

```{r}
library(raw)
data("comauto")

tbl_reserve <- comauto %>% 
  filter(DevelopmentYear < 1997) %>% 
  group_by(Company, AccidentYear) %>% 
  arrange(Lag, .by_group = TRUE) %>% 
  ungroup() %>% 
  mutate(
      PriorPaid = dplyr::lag(CumulativePaid)
    , IncrementalPaid = CumulativePaid - PriorPaid
  ) %>% 
  filter(Lag != 1) %>% 
  mutate(LagFactor = as.factor(Lag))

fit_reserves <- lm(
    CumulativePaid ~ 0 + PriorPaid:LagFactor
  , data = tbl_reserve
)

summary(fit_reserves)
```

## Reserves

```{r}
fit_reserves_inc <- lm(
    IncrementalPaid ~ 0 + PriorPaid:LagFactor
  , data = tbl_reserve
)
summary(fit_reserves_inc)
```

## Coefficient significance

```{r echo = TRUE}
tbl_linear <- tbl_linear %>% 
  mutate(
      exposure = x + rnorm(sims)
    , olep = exposure + rnorm(sims)
    , ep = olep + rnorm(sims)
  )
```

## Fit exposure

```{r}
fit_exposure <- lm(
  y ~ 1 + exposure
  , data = tbl_linear
)
summary(fit_exposure)
```

## Fit olep

```{r}
fit_olep <- lm(
    y ~ 1 + olep
  , data = tbl_linear
)
summary(fit_olep)
```

## Fit ep

```{r}
fit_ep <- lm(
    y ~ 1 + ep
  , data = tbl_linear
)
summary(fit_ep)
```

## Coefficient significance

When we don't know where the observation came from, maximum likelihood lets us reason about potential origins.

Maximum likelihood also lets us know what we don't know.

## Deviance

Deviance is twice the log-likelihood

```{r}
summary(fit_poisson)
```

##

```{r}
tbl_poisson <- tbl_poisson %>% 
  mutate(noise = rnorm(sims_poisson, 0, 10))

fit_poisson_2 <- glm(
    y ~ 1 + noise
  , data = tbl_poisson
  , family = poisson(link = 'identity'))
summary(fit_poisson_2)
```

# Must we use a distribution?

## Short answer

>- No

## Long answer

Gauss, Galton and Legendre were (mostly) uninterested in presuming a functional form $\therefore$ least squares estimation. 

Compare with Buhlmann and origins of credibility.

## Regularization - Lasso

$$RSS(\lambda, c)=\sum_{i=1}^n{(y_i-\hat{y_i})^2}+\lambda\sum_{j=1}^p{|\beta_j|}$$

Lasso - 

## Ridge

$$RSS(\lambda, c)=\sum_{i=1}^n{(y_i-\hat{y_i})^2}+\lambda\sum_{j=1}^p{\beta_j^2}$$

##

* Ridges reduces
* Lasso liminates

# Wrapping up

## All the other stuff I didn't mention

* **Hierearchical Models!!**
* Data pre-processing
    * Polynomial
    * Log
    * Centering
    * Other
* Time series
* Heteroskedasticity
* Stochastic coefficients --> Chain Ladder loss reserving!!

## What do these things have in common?

- Amy Winehouse - Rehab
- U2 - Still Haven't Found What I'm Looking For
- Tracy Chapman - Give Me One Reason
- Prince - Purple Rain
- The Clash - Should I Stay or Should I Go?
- KC & The Sunshine Band - Boogie Shoes
- Steelers Wheel - Stuck in the Middle With You
- Pink Floyd - Money
- Led Zeppelin - Rock and Roll
- James Brown - I Got You (I Feel Good)
- The Beatles - Can't Buy Me Love
- Johnny Cash - Folsom Prison Blues
- Miles Davis - All Blues
- Chuck Berry - Johnny B. Goode
- Robert Johnson - Sweet Home Chicago

## What do these things have in common?

* OLS
* GLM
    * Logistic
    * Poisson
    * Tweedie
* Time series
* Regularization
* Lasso

##

$$\beta_0 + \sum_{j=1}^{p}{\beta_j * x_{ij}}$$

## What do these things have in common?

* Linear combination of (transformed) predictors
* Distributional assumption (most of the time)
* Performance assessment of predictors
* Testable statistical hypotheses
    * Distributional form
    * Independent error terms
    * Predictor significance
* Assessment of residuals

## Thank you!

## References

* pirategrunt.com/linear_blues/
* https://musiciantuts.com/12-bar-blues-songs/
* http://the12barbluesaframework.blogspot.com/2009/04/introduction-to-12-bar-blues.html
* McCullough and Nelder
* Curtis Gary Dean
* R in a Nutshell